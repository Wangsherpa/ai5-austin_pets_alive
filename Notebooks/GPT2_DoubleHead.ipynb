{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GPT2 DoubleHead",
      "provenance": [],
      "collapsed_sections": [
        "Q-T6U0SwXug1",
        "FYZvlg22YRJl",
        "jq5OEAviX6gM",
        "bMYXTUxTYqda",
        "hMbqhVAmY0Se",
        "Wlc49nXVwdDf",
        "GHDN94Km85Z9",
        "-pgnHu_EgHPh",
        "iDdXTvcvgKw4",
        "MFnz-qTygL4T"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "903717b1249b4d0e870e32ab3ccd54b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e010bc294ba2423fa9db7e07e07531ca",
              "IPY_MODEL_f8064b31224e474abc244c744e0105d9",
              "IPY_MODEL_db457cb504464f1baa6a29ac0e4d2756"
            ],
            "layout": "IPY_MODEL_59a67e6a524f4be8b0c1a5173672d762"
          }
        },
        "e010bc294ba2423fa9db7e07e07531ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c382570280c0456b85e69da54cda9e9a",
            "placeholder": "​",
            "style": "IPY_MODEL_f695e198f71044a79e2d98896a6d4d7a",
            "value": "100%"
          }
        },
        "f8064b31224e474abc244c744e0105d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24352302a0414da980d0d0da995571ed",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ac02dcbe3e149318787f910ab78757e",
            "value": 50
          }
        },
        "db457cb504464f1baa6a29ac0e4d2756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9829859bd00416e998a84df0ce5532f",
            "placeholder": "​",
            "style": "IPY_MODEL_5c6ea7c80a794581b7be17a5bd21bd95",
            "value": " 50/50 [00:01&lt;00:00,  1.22s/it]"
          }
        },
        "59a67e6a524f4be8b0c1a5173672d762": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c382570280c0456b85e69da54cda9e9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f695e198f71044a79e2d98896a6d4d7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24352302a0414da980d0d0da995571ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ac02dcbe3e149318787f910ab78757e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9829859bd00416e998a84df0ce5532f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c6ea7c80a794581b7be17a5bd21bd95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be843c5f273e47e0b1e114baf326b013": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd5d6dd9c12b40a4a453cbc2e97db76a",
              "IPY_MODEL_89de50cada3a4bafa988383b9b20f40c",
              "IPY_MODEL_629e84f51e34433aaffb58d7903de2ab"
            ],
            "layout": "IPY_MODEL_79e7f82d1fb64e0187d1b84e4bfe655e"
          }
        },
        "dd5d6dd9c12b40a4a453cbc2e97db76a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe40e24470834374be7567ccdef2ea5d",
            "placeholder": "​",
            "style": "IPY_MODEL_1cca48263482422e97af6ce63f9dcb45",
            "value": "Epoch 1 of 1: 100%"
          }
        },
        "89de50cada3a4bafa988383b9b20f40c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45838cb4a9e741c5bc751130f96c369b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_930f3d84a09a4aba9478389a006505a1",
            "value": 1
          }
        },
        "629e84f51e34433aaffb58d7903de2ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d37005083e484eda835f287f50fbf976",
            "placeholder": "​",
            "style": "IPY_MODEL_2130d9c00a5147a08b44bf10d58ca52d",
            "value": " 1/1 [01:38&lt;00:00, 98.19s/it]"
          }
        },
        "79e7f82d1fb64e0187d1b84e4bfe655e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe40e24470834374be7567ccdef2ea5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cca48263482422e97af6ce63f9dcb45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45838cb4a9e741c5bc751130f96c369b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "930f3d84a09a4aba9478389a006505a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d37005083e484eda835f287f50fbf976": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2130d9c00a5147a08b44bf10d58ca52d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c5b28a9143040cd8113facae72c4950": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b10adad3a474f6cb0b79e4a5ead6e06",
              "IPY_MODEL_366e9d5d9527413d9397716161660f37",
              "IPY_MODEL_f4fefc03e7924ff0bcdb040a793b1cc3"
            ],
            "layout": "IPY_MODEL_db43dfaf04d641578fbc55b004c018f9"
          }
        },
        "3b10adad3a474f6cb0b79e4a5ead6e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c1892b5933246479630258ca4372738",
            "placeholder": "​",
            "style": "IPY_MODEL_6519125c286642929cbb2f0de8c7ee05",
            "value": "Running Epoch 0 of 1: 100%"
          }
        },
        "366e9d5d9527413d9397716161660f37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af327e57390d46c1ad3e83a1e41e1f86",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0db027424ee140598e7045ffa11bc5e6",
            "value": 100
          }
        },
        "f4fefc03e7924ff0bcdb040a793b1cc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4680d493c3ab41808c3c63b331bf60ce",
            "placeholder": "​",
            "style": "IPY_MODEL_5d8f60fc535a4a77bae9d38e9f9ae317",
            "value": " 100/100 [01:38&lt;00:00,  1.01it/s]"
          }
        },
        "db43dfaf04d641578fbc55b004c018f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c1892b5933246479630258ca4372738": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6519125c286642929cbb2f0de8c7ee05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af327e57390d46c1ad3e83a1e41e1f86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0db027424ee140598e7045ffa11bc5e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4680d493c3ab41808c3c63b331bf60ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d8f60fc535a4a77bae9d38e9f9ae317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZFg22gRjUSA",
        "outputId": "b39a970a-0943-46d6-91d9-c93a83cdb17b"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = 'ai5-c1-group1'\n",
        "!gcloud config set project {project_id}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "\n",
            "\n",
            "To take a quick anonymous survey, run:\n",
            "  $ gcloud survey\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1gRM_dKVt1a",
        "outputId": "fa239312-f036-451e-ee78-863c097b622a"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.14.0-py3-none-any.whl (290 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 17.3 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 30 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 40 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 51 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 61 kB 2.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 71 kB 2.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 81 kB 3.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 92 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 102 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 112 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 122 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 133 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 143 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 153 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 163 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 174 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 184 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 194 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 204 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 215 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 225 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 235 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 245 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 256 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 266 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 276 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 286 kB 2.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 290 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 47.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting huggingface-hub<0.1.0,>=0.0.19\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 49.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 47.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 48.8 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 51.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, xxhash, huggingface-hub, datasets\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.14.0 fsspec-2021.10.1 huggingface-hub-0.0.19 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.0\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 4.1 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 45.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 50.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 35.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYZvlg22YRJl"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHGROZdZYTQl"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import tarfile\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "import math\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from argparse import ArgumentParser\n",
        "from subprocess import call\n",
        "import textwrap\n",
        "\n",
        "from collections import defaultdict\n",
        "from multiprocessing import Pool\n",
        "from tqdm.auto import tqdm, trange\n",
        "from itertools import chain\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda import amp\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import GPT2Config, GPT2LMHeadModel, GPT2DoubleHeadsModel, GPT2Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq5OEAviX6gM"
      },
      "source": [
        "#### Setup Logger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgaLWUyoX68Y"
      },
      "source": [
        "# Setup Logger\n",
        "if '__file__' not in globals():\n",
        "  __file__ = \".\"\n",
        "logger = logging.getLogger(__file__)\n",
        "\n",
        "# Logger config\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMYXTUxTYqda"
      },
      "source": [
        "#### Verify Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qkc7l_wmYq95",
        "outputId": "b50a53c0-ec61-4f42-83b9-6958e64aec7a"
      },
      "source": [
        "logger.info('__Python VERSION: %s', sys.version)\n",
        "logger.info(\"torch version: %s\", torch.__version__)\n",
        "logger.info('CUDNN VERSION: %s', torch.backends.cudnn.version())\n",
        "logger.info('Number CUDA Devices: %s', torch.cuda.device_count())\n",
        "cuda_available = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if cuda_available else \"cpu\")\n",
        "device_count = 0\n",
        "\n",
        "if cuda_available:\n",
        "  device_count = torch.cuda.device_count()\n",
        "  logger.info('Devices:')\n",
        "  logger.info('Active CUDA Device: %s', torch.cuda.current_device())\n",
        "  logger.info('Available device count: %s', device_count)\n",
        "  logger.info('Current cuda device: %s', torch.cuda.current_device())\n",
        "else:\n",
        "  logger.info('No CUDA Devices are available')\n",
        "\n",
        "logger.info('Device: %s', device)\n",
        "  \n",
        "\n",
        "# nvidia-smi\n",
        "call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:.:__Python VERSION: 3.7.12 (default, Sep 10 2021, 00:21:48) \n",
            "[GCC 7.5.0]\n",
            "INFO:.:torch version: 1.9.0+cu111\n",
            "INFO:.:CUDNN VERSION: 8005\n",
            "INFO:.:Number CUDA Devices: 1\n",
            "INFO:.:Devices:\n",
            "INFO:.:Active CUDA Device: 0\n",
            "INFO:.:Available device count: 1\n",
            "INFO:.:Current cuda device: 0\n",
            "INFO:.:Device: cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMbqhVAmY0Se"
      },
      "source": [
        "#### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8h7rBETY1WP"
      },
      "source": [
        "def download_file(packet_url, base_path=\"\", extract=False, headers=None):\n",
        "  if base_path != \"\":\n",
        "    if not os.path.exists(base_path):\n",
        "      os.mkdir(base_path)\n",
        "  packet_file = os.path.basename(packet_url)\n",
        "  with requests.get(packet_url, stream=True, headers=headers) as r:\n",
        "      r.raise_for_status()\n",
        "      with open(os.path.join(base_path,packet_file), 'wb') as f:\n",
        "          for chunk in r.iter_content(chunk_size=8192):\n",
        "              f.write(chunk)\n",
        "  \n",
        "  if extract:\n",
        "    if packet_file.endswith(\".zip\"):\n",
        "      with zipfile.ZipFile(os.path.join(base_path,packet_file)) as zfile:\n",
        "        zfile.extractall(base_path)\n",
        "    else:\n",
        "      packet_name = packet_file.split('.')[0]\n",
        "      with tarfile.open(os.path.join(base_path,packet_file)) as tfile:\n",
        "        tfile.extractall(base_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmg4Oq31Gl1S"
      },
      "source": [
        "## **<font color=\"darkred\">GPT2 Double Head Model</font>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNMQ1VubG3sG"
      },
      "source": [
        "#### Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfxdC7mlG1eh"
      },
      "source": [
        "We have seen how a Question Answering model works, we also saw how a Language generation model works. Let's attempt to combine some these ideas from the two models into one that can both answer questions as well as generate them. For this we will extend the GPT2 model.\n",
        "\n",
        "**Causal Transformer**: \n",
        "\n",
        "We saw that GPT2 is the made up of only the Decoder with stacked transformer blocks. Also the model predicts words using only words from the left context. So if we look at our example on Emma.\n",
        "<img src=\"https://storage.googleapis.com/public_colab_images/nlp/gpt2/causaltransformer02.png\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX3GNzuqmMHG"
      },
      "source": [
        "**Double Head Model**: \n",
        "\n",
        "Now how do we adapt this language model into a dialog task? In a question answering model we had to feed in a context and the model returned an answer. The language model generated text based on previous words. So if use the GPT2 model as a base and for the input we add some context to the data such as:\n",
        "- Information about the dog, or its `persona`\n",
        "- The `history` of the dialogue with the user\n",
        "- The `answer` of the dog\n",
        "\n",
        "And as a head we add:\n",
        "- Language Model Head\n",
        "- Multiple Choice Head\n",
        "\n",
        "The GPT2 has by default one language model head which takes the hidden states from the final transform block and pass it to a linear layer to compute the logits. We then add another head called mutiple choice head, which takes the hidden states from the final transform block and summarizes the sequences to a single vector of a sequence hidden states. This could be done using `last` which is to take the last token hidden state, or `first` which is to take the first token hidden state, or `mean` which is to take the mean of all tokens hidden states.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/public_colab_images/nlp/gpt2/gpt2doubleheadmodel.png\" />\n",
        "\n",
        "\n",
        "**Word Embeddings**: Word embeddings are where each word in the dataset is mapped to a numberical vector. Each of these vector has a sense of context between the words. So for exmaple words with simialr meaning or concepts come together in the vector space.\n",
        "\n",
        "**Positional Embedding**: A transformer based model has no sense of the sequence of an input. So to give the model some sense of order we add a piece of information to each word about its position in the sentence. So positional embedding is a n-dimensional vector that contains information about a specific position in a sentence.\n",
        "\n",
        "**Segment Embedding**: Our input consists of persona, history, and answer. So we want add information about each segment in the input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6aszzg3GtLs"
      },
      "source": [
        "**Finetuning Options**: \n",
        "\n",
        "There are multiple options to perform transfer learning and finetuing for our final dialog model:\n",
        "<img src=\"https://storage.googleapis.com/public_colab_images/nlp/gpt2/gpt2dhfinetuning01.png\" width=\"800\"/>\n",
        "\n",
        "- PERSONA-CHAT dataset size - 17,000\n",
        "- Our dog dataset (small) 800"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB4-lSRsyav7"
      },
      "source": [
        "#### Dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ffgBHw3y7eb"
      },
      "source": [
        "##### Example from PERSONA-CHAT dataset\n",
        "\n",
        "PERSONA-CHAT is a large dataset of dialogs which was created by crowdsourcing personality sentences and asking paired crowd workers to chit-chat while playing the part of a given character"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57VDh1nWyjtA"
      },
      "source": [
        "```\n",
        "{\n",
        "   \"personality\":[\n",
        "      \"my mom is my best friend .\",\n",
        "      \"i have four sisters .\",\n",
        "      \"i believe that mermaids are real .\",\n",
        "      \"i love iced tea .\"\n",
        "   ],\n",
        "   \"utterances\":[\n",
        "      {\n",
        "         \"candidates\":[\n",
        "            \"there was one person better than me , but i will keep trying to pass\",\n",
        "            \"oh that's a yummy jpb\",\n",
        "            \"i take it you are not getting along with him ?\",\n",
        "            \"...\",\n",
        "            \"good . where are you from ?\",\n",
        "            \"right now i am doing an ocean liner .\",\n",
        "            \"i am spending time with my 4 sisters what are you up to\"\n",
        "         ],\n",
        "         \"history\":[\n",
        "            \"hi , how are you doing today ?\"\n",
        "         ]\n",
        "      },\n",
        "      {\n",
        "         \"candidates\":[\n",
        "            \"that would be great . what do you do on the weekends ?\",\n",
        "            \"i am sorry to hear about that . i am not married .\",\n",
        "            \"...\",\n",
        "            \"ah manic depressive . how to you cope ? meditation ? any hobbies ?\",\n",
        "            \"hey ! what kind of music are your into ?\",\n",
        "            \"that is a good show i watch that while drinking iced tea\"\n",
        "         ],\n",
        "         \"history\":[\n",
        "            \"hi , how are you doing today ?\",\n",
        "            \"i am spending time with my 4 sisters what are you up to\",\n",
        "            \"wow , four sisters . just watching game of thrones .\"\n",
        "         ]\n",
        "      },\n",
        "      {\n",
        "         \"candidates\":[\n",
        "            \"very long , she was with me when i colored my hair pink\",\n",
        "            \"i like being alone and hitchhiking\",\n",
        "            \"hi , how are you doing today ?\",\n",
        "            \"actually been good and better than what i expected\",\n",
        "            \"...\",\n",
        "            \"my name is charlie . what kind of earrings ?\",\n",
        "            \"of course . i'm listening .\",\n",
        "            \"i'm a researcher i'm researching the fact that mermaids are real\"\n",
        "         ],\n",
        "         \"history\":[\n",
        "            \"hi , how are you doing today ?\",\n",
        "            \"i am spending time with my 4 sisters what are you up to\",\n",
        "            \"wow , four sisters . just watching game of thrones .\",\n",
        "            \"that is a good show i watch that while drinking iced tea\",\n",
        "            \"i agree . what do you do for a living ?\"\n",
        "         ]\n",
        "      }\n",
        "   ]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCtV0BwmzpA2"
      },
      "source": [
        "##### Example dialog dataset generated for dogs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MOfTq0_zuAp"
      },
      "source": [
        "The persona dataset for dogs were generated using some basic metadata we have about the dogs. Nothing fancy but this helps to test out our dialog model\n",
        "\n",
        "```\n",
        "{\n",
        "   \"personality\":[\n",
        "      \"I am Emma\",\n",
        "      \"I am a Dog\",\n",
        "      \"My gender is Female\",\n",
        "      \"My weight is 55.0\",\n",
        "      \"I was born on 2009\",\n",
        "      \"I am 11.0 years old\",\n",
        "      \"My breed is Retriever, Yellow Labrador\",\n",
        "      \"My color is White\",\n",
        "      \"I am house trained\"\n",
        "   ],\n",
        "   \"utterances\":[\n",
        "      {\n",
        "         \"candidates\":[\n",
        "            \"i do , but mostly after work with the boys\",\n",
        "            \"not if you inherit it and then reinvest . that s what trump did . lots do .\",\n",
        "            \"...\",\n",
        "            \"it it not a great experience , let me tell you .\",\n",
        "            \"woof woof . i'm feeling great!\"\n",
        "         ],\n",
        "         \"history\":[\n",
        "            \"hi , how are you ?\"\n",
        "         ]\n",
        "      },\n",
        "      {\n",
        "         \"candidates\":[\n",
        "            \"hi , how are you doing today ?\",\n",
        "            \"what do you read ? i just graduated college . i was chicago for school .\",\n",
        "            \"...\"\n",
        "            \"no i'm not right now but will be soon\",\n",
        "            \"my name is Emma\"\n",
        "         ],\n",
        "         \"history\":[\n",
        "            \"hi , how are you ?\",\n",
        "            \"woof woof . i'm feeling great!\",\n",
        "            \"what is your name ?\"\n",
        "         ]\n",
        "      },\n",
        "      {\n",
        "         \"candidates\":[\n",
        "            \"nice ! i live near the gulf of mexico . youre a doctor ?\",\n",
        "            \"i work at the ymca and i'm a member too .\",\n",
        "            \"you must be creative , people like you do well in my field at ibm .\",\n",
        "            \"...\"\n",
        "            \"hi , how are you today\",\n",
        "            \"i am a Dog\"\n",
        "         ],\n",
        "         \"history\":[\n",
        "            \"hi , how are you ?\",\n",
        "            \"woof woof . i'm feeling great!\",\n",
        "            \"what is your name ?\",\n",
        "            \"my name is Emma\",\n",
        "            \"what are you ?\"\n",
        "         ]\n",
        "      },\n",
        "      {\n",
        "         \"candidates\":[\n",
        "            \"great . how are you doing ?\",\n",
        "            \"that is a good way to put it .\",\n",
        "            \"...\"\n",
        "            \"it happens . i find them all the time at my office .\",\n",
        "            \"i am Female\"\n",
        "         ],\n",
        "         \"history\":[\n",
        "            \"hi , how are you ?\",\n",
        "            \"woof woof . i'm feeling great!\",\n",
        "            \"what is your name ?\",\n",
        "            \"my name is Emma\",\n",
        "            \"what are you ?\",\n",
        "            \"i am a Dog\",\n",
        "            \"what is your gender ?\"\n",
        "         ]\n",
        "      }\n",
        "   ]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH6jgc9PG82c"
      },
      "source": [
        "#### Load Pretrained Model/Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-zeZ7nP8jy5"
      },
      "source": [
        "We already have a pretrained model that was trained on the PERSON-CHAT dataset for 1 epoch (Takes around 2 hours)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st43ZprOHDZR",
        "outputId": "2ff5c6a6-08de-40c8-a76a-029311408e6e"
      },
      "source": [
        "# Download pretrained model \n",
        "model_url = \"https://storage.googleapis.com/artifacts.ai5-c1-group1.appspot.com/data/transferlearning_gpt2doublehead.zip\"\n",
        "start_time = time.time()\n",
        "download_file(model_url, base_path=\"models\", extract=True)\n",
        "execution_time = (time.time() - start_time)/60.0\n",
        "logger.info(\"Download execution time (mins): %s\",execution_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:.:Download execution time (mins): 0.21209884484608968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6BSkD0nHMZb"
      },
      "source": [
        "# Load trained model\n",
        "# model = GPT2DoubleHeadsModel.from_pretrained(\"gpt2\")\n",
        "# # Convert model parameter tensors to CUDA tensors\n",
        "# model.to(device)\n",
        "# # Load trained Tokenizer\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load trained model\n",
        "model = GPT2DoubleHeadsModel.from_pretrained('/content/models/transferlearning_gpt2doublehead')\n",
        "# Convert model parameter tensors to CUDA tensors\n",
        "model.to(device)\n",
        "# Load trained Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"/content/models/transferlearning_gpt2doublehead\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rsy_MyyH0nN"
      },
      "source": [
        "#### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6-EQvda9ql1"
      },
      "source": [
        "# Tokens specific for GPT2 Double Head Model\n",
        "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
        "ATTR_TO_SPECIAL_TOKEN = {\n",
        "    \"bos_token\": \"<bos>\",\n",
        "    \"eos_token\": \"<eos>\",\n",
        "    \"pad_token\": \"<pad>\",\n",
        "    \"additional_special_tokens\": [\"<speaker1>\", \"<speaker2>\"],\n",
        "}\n",
        "MODEL_INPUTS = [\"input_ids\", \"mc_token_ids\", \"lm_labels\", \"mc_labels\", \"token_type_ids\"]\n",
        "PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHDN94Km85Z9"
      },
      "source": [
        "##### Util Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4141fs_H3Xm"
      },
      "source": [
        "# Utils for tokenization & data preparation\n",
        "process_count = 1\n",
        "multiprocessing_chunksize = 500\n",
        "\n",
        "def tokenize_multi(data):\n",
        "  obj, tokenizer = data\n",
        "  if isinstance(obj, str):\n",
        "      return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
        "  if isinstance(obj, dict):\n",
        "      return dict((n, tokenize_multi((o, tokenizer))) for n, o in obj.items())\n",
        "  return list(tokenize_multi((o, tokenizer)) for o in obj)\n",
        "\n",
        "def tokenize(obj):\n",
        "  if isinstance(obj, str):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
        "  if isinstance(obj, dict):\n",
        "    return dict((n, tokenize(o)) for n, o in obj.items())\n",
        "\n",
        "  data = [(d, tokenizer) for d in obj]\n",
        "  with Pool(process_count) as p:\n",
        "    tokenized_data = list(\n",
        "        tqdm(p.imap(tokenize_multi, data, chunksize=multiprocessing_chunksize), total=len(data))\n",
        "    )\n",
        "  return tokenized_data\n",
        "\n",
        "def build_input_from_segments(persona, history, reply, tokenizer, lm_labels=False, with_eos=True):\n",
        "  \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
        "  bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
        "  sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])]\n",
        "  sequence = [sequence[0]] + [\n",
        "      [speaker2 if (len(sequence) - i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])\n",
        "  ]\n",
        "  instance = {}\n",
        "  instance[\"input_ids\"] = list(chain(*sequence))\n",
        "  instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
        "  instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
        "  instance[\"lm_labels\"] = [-100] * len(instance[\"input_ids\"])\n",
        "  if lm_labels:\n",
        "      instance[\"lm_labels\"] = ([-100] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
        "  return instance\n",
        "\n",
        "def pad_dataset(dataset, padding=0):\n",
        "  \"\"\" Pad the dataset. This could be optimized by defining a Dataset class and padding at the batch level,\n",
        "  but this is simpler. \"\"\"\n",
        "  max_l = max(len(x) for x in dataset[\"input_ids\"])\n",
        "  for name in PADDED_INPUTS:\n",
        "      dataset[name] = [x + [padding if name != \"lm_labels\" else -100] * (max_l - len(x)) for x in dataset[name]]\n",
        "  return dataset\n",
        "\n",
        "def prepare_datasets(dataset, num_candidates):\n",
        "  datasets = defaultdict(list)\n",
        "  for dialog in dataset:\n",
        "    persona = dialog[\"personality\"].copy()\n",
        "    for _ in range(args.personality_permutations):\n",
        "      for utterance in dialog[\"utterances\"]:\n",
        "          history = utterance[\"history\"][-(2 * args.max_history + 1) :]\n",
        "          for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\n",
        "              lm_labels = bool(j == num_candidates - 1)\n",
        "              instance = build_input_from_segments(persona, history, candidate, tokenizer, lm_labels)\n",
        "              for input_name, input_array in instance.items():\n",
        "                  datasets[input_name].append(input_array)\n",
        "          datasets[\"mc_labels\"].append(num_candidates - 1)\n",
        "          datasets[\"n_candidates\"] = num_candidates\n",
        "      # permuted personalities\n",
        "      persona = [persona[-1]] + persona[:-1]\n",
        "  return datasets\n",
        "\n",
        "def top_filtering(logits, top_k=0.0, top_p=0.9, threshold=-float(\"Inf\"), filter_value=-float(\"Inf\")):\n",
        "  top_k = min(top_k, logits.size(-1))\n",
        "  if top_k > 0:\n",
        "      # Remove all tokens with a probability less than the last token in the top-k tokens\n",
        "      indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "      logits[indices_to_remove] = filter_value\n",
        "\n",
        "  if top_p > 0.0:\n",
        "      # Compute cumulative probabilities of sorted tokens\n",
        "      sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "      cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "      # Remove tokens with cumulative probability above the threshold\n",
        "      sorted_indices_to_remove = cumulative_probabilities > top_p\n",
        "      # Shift the indices to the right to keep also the first token above the threshold\n",
        "      sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "      sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "      # Back to unsorted indices and set them to -infinity\n",
        "      indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "      logits[indices_to_remove] = filter_value\n",
        "\n",
        "  indices_to_remove = logits < threshold\n",
        "  logits[indices_to_remove] = filter_value\n",
        "\n",
        "  return logits\n",
        "\n",
        "def generate_sequence(personality, history, tokenizer, model, current_output=None):\n",
        "  with torch.no_grad():\n",
        "    with amp.autocast():\n",
        "      special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
        "      if current_output is None:\n",
        "          current_output = []\n",
        "\n",
        "      # Args\n",
        "      max_length = 20\n",
        "      temperature = 0.7\n",
        "      top_k = 0\n",
        "      top_p = 0.9\n",
        "      do_sample = True\n",
        "      min_length = 1\n",
        "\n",
        "      for i in range(max_length):\n",
        "          instance = build_input_from_segments(\n",
        "              personality, history, current_output, tokenizer, with_eos=False\n",
        "          )\n",
        "\n",
        "          input_ids = torch.tensor(instance[\"input_ids\"], device=device).unsqueeze(0)\n",
        "          token_type_ids = torch.tensor(instance[\"token_type_ids\"], device=device).unsqueeze(0)\n",
        "\n",
        "          logits = model(input_ids, token_type_ids=token_type_ids)\n",
        "          logits = logits[0]\n",
        "\n",
        "          logits = logits[0, -1, :] / temperature\n",
        "          logits = top_filtering(logits, top_k=top_k, top_p=top_p)\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "          prev = torch.topk(probs, 1)[1] if not do_sample else torch.multinomial(probs, 1)\n",
        "          if i < min_length and prev.item() in special_tokens_ids:\n",
        "              while prev.item() in special_tokens_ids:\n",
        "                  if probs.max().item() == 1:\n",
        "                      break  # avoid infinite loop\n",
        "                  prev = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "          if prev.item() in special_tokens_ids:\n",
        "              break\n",
        "          current_output.append(prev.item())\n",
        "\n",
        "  return current_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSGs1lApHc_P"
      },
      "source": [
        "#### Without finetuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2pn43tpHfMS"
      },
      "source": [
        "# Personality\n",
        "test_personality=[\n",
        "  'I am Yash',\n",
        "  'I am a human',\n",
        "  'My gender is male',\n",
        "  'My weight is 53.0',\n",
        "  'I was born on 2009',\n",
        "  'I am 11 years old',\n",
        "  'My breed is Retriever, Yellow Labrador',\n",
        "  'My color is White/Yellow',\n",
        "  'I am house trained','i like to play with toys']\n",
        "\n",
        "# History\n",
        "test_history = [\n",
        "    \"Hi\",\n",
        "    \"woof woof\"\n",
        "]\n",
        "print(test_personality)\n",
        "print(test_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKbH7n3BHhdk",
        "outputId": "aca81659-3dbb-4f9d-99cd-ad736bf540a2"
      },
      "source": [
        "# New chat message\n",
        "test_message = \"what is your name?\"\n",
        "\n",
        "# Tokenize\n",
        "personality = [tokenizer.encode(s.lower()) for s in test_personality]\n",
        "history = [tokenizer.encode(s) for s in test_history]\n",
        "history.append(tokenizer.encode(test_message))\n",
        "# Generate output\n",
        "output = generate_sequence(personality, history, tokenizer, model)\n",
        "output_text = tokenizer.decode(output, skip_special_tokens=True)\n",
        "\n",
        "print(\"Question:\")\n",
        "print(test_message)\n",
        "print(\"Answer:\")\n",
        "print(output_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:\n",
            "what is your name?\n",
            "Answer:\n",
            "i am ryan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vpK9LyZKMSf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7505DPQnKp4"
      },
      "source": [
        "#### With Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NML4bJfqqsQB",
        "outputId": "d836ed1a-f4e3-467a-e869-d6733c739a6d"
      },
      "source": [
        "# Setup Model Training Arguments\n",
        "parser = ArgumentParser()\n",
        "parser.add_argument(\"--epochs\", type=int, default=1, help=\"Number of training epochs\")\n",
        "parser.add_argument(\"--train_batch_size\", type=int, default=4, help=\"Batch size for training\")\n",
        "parser.add_argument(\"--validation_batch_size\", type=int, default=4, help=\"Batch size for validation\")\n",
        "parser.add_argument(\"--num_candidates\", type=int, default=2, help=\"Number of candidates for training\")\n",
        "parser.add_argument(\"--max_history\", type=int, default=2, help=\"Number of previous exchanges to keep in history\")\n",
        "parser.add_argument(\"--personality_permutations\", type=int, default=1, help=\"Number of permutations of personality sentences\")\n",
        "parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1, help=\"Accumulate gradients on several steps\")\n",
        "parser.add_argument(\"--learning_rate\", type=float, default=1e-05, help=\"Learning rate\")\n",
        "parser.add_argument(\"--lm_coef\", type=float, default=2.0, help=\"LM loss coefficient\")\n",
        "parser.add_argument(\"--mc_coef\", type=float, default=1.0, help=\"Multiple-choice loss coefficient\")\n",
        "parser.add_argument(\"--weight_decay\", type=float, default=0.0, help=\"Optimizer weight decay\")\n",
        "parser.add_argument(\"--warmup_steps\", type=int, default=0, help=\"Number of warmup steps\")\n",
        "parser.add_argument(\"--warmup_ratio\", type=float, default=0.06, help=\"Warmup ratio\")\n",
        "parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08, help=\"Adam optimizer epsilon\")\n",
        "parser.add_argument(\"--verbose\", type=int, default=1, help=\"Verbose logging\")\n",
        "parser.add_argument(\"--max_norm\", type=float, default=1.0, help=\"Clipping gradient norm\")\n",
        "parser.add_argument(\"--model_dir\", type=str, default=\"model_outputs\", help=\"Path to save model\")\n",
        "\n",
        "args = parser.parse_args(\"\")\n",
        "logger.info(\"Arguments: %s\", args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:.:Arguments: Namespace(adam_epsilon=1e-08, epochs=1, gradient_accumulation_steps=1, learning_rate=4e-05, lm_coef=2.0, max_history=2, max_norm=1.0, mc_coef=1.0, model_dir='model_outputs', num_candidates=2, personality_permutations=1, train_batch_size=4, validation_batch_size=4, verbose=1, warmup_ratio=0.06, warmup_steps=0, weight_decay=0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXEnqi_GnQYk"
      },
      "source": [
        "# # If you want to try to fine tune from GPT2 pretrained weights directly here is the code\n",
        "# # Model\n",
        "# model = GPT2DoubleHeadsModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# # Tokenizer\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# # Add special tokens to the tokenizer and model\n",
        "# orig_num_tokens = len(tokenizer.encoder)\n",
        "# # Add special tokens\n",
        "# num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN)\n",
        "# if num_added_tokens > 0:\n",
        "#   model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens)\n",
        "\n",
        "# # Convert model parameter tensors to CUDA tensors\n",
        "# model.to(device)\n",
        "\n",
        "# print(\"model type:\",type(model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWIdI0gwmy5d"
      },
      "source": [
        "#### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqrm9Yo-UUs3"
      },
      "source": [
        "dataset_url = \"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\"\n",
        "start_time = time.time()\n",
        "download_file(dataset_url, base_path=\"datasets\", extract=False)\n",
        "execution_time = (time.time() - start_time)/60.0\n",
        "\n",
        "dataset_url = \"https://storage.googleapis.com/artifacts.ai5-c1-group1.appspot.com/data/personadogchat.json\"\n",
        "start_time = time.time()\n",
        "download_file(dataset_url, base_path=\"datasets\", extract=False)\n",
        "execution_time = (time.time() - start_time)/60.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWSOo8uGn4mi"
      },
      "source": [
        "personachat_file = os.path.join(\"datasets\",\"personadogchat.json\")\n",
        "with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
        "  personachat = json.loads(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368,
          "referenced_widgets": [
            "903717b1249b4d0e870e32ab3ccd54b0",
            "e010bc294ba2423fa9db7e07e07531ca",
            "f8064b31224e474abc244c744e0105d9",
            "db457cb504464f1baa6a29ac0e4d2756",
            "59a67e6a524f4be8b0c1a5173672d762",
            "c382570280c0456b85e69da54cda9e9a",
            "f695e198f71044a79e2d98896a6d4d7a",
            "24352302a0414da980d0d0da995571ed",
            "8ac02dcbe3e149318787f910ab78757e",
            "a9829859bd00416e998a84df0ce5532f",
            "5c6ea7c80a794581b7be17a5bd21bd95"
          ]
        },
        "id": "uiDdH5QTm1c9",
        "outputId": "dce3cc56-9bf3-4343-9e22-dbfe04b4afe0"
      },
      "source": [
        "subset_size = 50\n",
        "# Tokenize dataset\n",
        "train_processed = tokenize(personachat[:subset_size])\n",
        "\n",
        "print(\"train count:\",len(train_processed))\n",
        "print(train_processed[:2])\n",
        "\n",
        "train_num_candidates = len(train_processed[0][\"utterances\"][0][\"candidates\"])\n",
        "if args.num_candidates > 0:\n",
        "  train_num_candidates = min(args.num_candidates, train_num_candidates)\n",
        "\n",
        "# Prepare dataset inputs & outputs\n",
        "train_processed = prepare_datasets(train_processed, train_num_candidates)\n",
        "print(\"After adding inputs/outputs:\")\n",
        "print(\"train_processed keys:\", train_processed.keys())\n",
        "print(\"input_ids:\",len(train_processed[\"input_ids\"][0]),train_processed[\"input_ids\"][0])\n",
        "print(\"token_type_ids:\",len(train_processed[\"token_type_ids\"][0]),train_processed[\"token_type_ids\"][0])\n",
        "print(\"mc_token_ids:\",len(train_processed[\"mc_token_ids\"]))\n",
        "print(\"lm_labels:\",len(train_processed[\"lm_labels\"][0]),train_processed[\"lm_labels\"][0])\n",
        "print(\"mc_labels:\",len(train_processed[\"mc_labels\"]))\n",
        "print(\"n_candidates:\",train_processed[\"n_candidates\"])\n",
        "\n",
        "# Pad datasets\n",
        "train_processed = pad_dataset(train_processed, padding=tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n",
        "print(\"After Padding:\")\n",
        "print(\"input_ids:\",len(train_processed[\"input_ids\"][0]),train_processed[\"input_ids\"][0])\n",
        "print(\"token_type_ids:\",len(train_processed[\"token_type_ids\"][0]),train_processed[\"token_type_ids\"][0])\n",
        "print(\"mc_token_ids:\",len(train_processed[\"mc_token_ids\"]))\n",
        "print(\"lm_labels:\",len(train_processed[\"lm_labels\"][0]),train_processed[\"lm_labels\"][0])\n",
        "print(\"mc_labels:\",len(train_processed[\"mc_labels\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "903717b1249b4d0e870e32ab3ccd54b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train count: 50\n",
            "[{'personality': [[3666, 1438, 318, 18966, 13], [3666, 2479, 318, 718, 13], [40, 716, 3290, 13], [3666, 5279, 318, 4048, 13], [40, 10164, 7192, 13, 18, 8059, 13], [40, 716, 1086, 623, 287, 3124, 13], [3666, 15939, 318, 4990, 380, 964, 13]], 'utterances': [{'candidates': [[72, 1101, 257, 7394, 764, 345, 389, 262, 352, 301, 1048, 326, 1312, 1297, 764, 1315, 837, 12877, 764], [72, 588, 477, 6982, 286, 2647, 837, 475, 716, 407, 845, 5385, 351, 27296], [72, 1101, 407, 6405, 290, 616, 3988, 389, 7334, 764, 703, 546, 345, 5633], [72, 1101, 1804, 880, 837, 703, 389, 345, 5633], [72, 18854, 374, 31562, 3881, 290, 4836, 764, 345, 5633], [72, 588, 284, 3124, 290, 1312, 5806, 257, 14335, 764], [72, 1654, 2911, 523, 5238, 588, 673, 15063, 1223, 2089], [43669, 837, 663, 825, 407, 14262, 764, 597, 3352, 329, 262, 5041, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0]], 'history': [[5303, 837, 703, 389, 345, 5633]]}, {'candidates': [[1219, 764, 466, 345, 423, 597, 17252, 5633], [42773, 4998, 466, 345, 423, 3988, 5633], [72, 787, 281, 7427, 27384, 46557, 530, 286, 616, 4004, 47950], [72, 588, 284, 787, 8242, 416, 35356], [72, 716, 2111, 284, 3613, 510, 523, 1312, 466, 407, 423, 284, 1011, 503, 10021, 764], [1219, 290, 1312, 1101, 1804, 922, 837, 1165, 764, 772, 996, 1312, 1053, 1239, 587, 27946, 416, 257, 2415, 764], [3919, 1312, 655, 588, 284, 307, 287, 3094, 1280, 9029, 764, 1312, 815, 923, 16035, 764], [5303, 837, 1312, 1842, 6899, 322, 6429, 837, 523, 644, 466, 345, 588, 284, 466, 329, 1257, 5633], [40, 716, 18966, 13]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633]]}, {'candidates': [[44460, 5145, 703, 1468, 389, 345, 5633, 1312, 1053, 257, 7684, 286, 3988], [19631, 880, 644, 389, 534, 3352, 329, 262, 5041, 5633], [270, 318, 290, 1312, 2050, 1099, 523, 1312, 760], [2188, 329, 340, 5145, 326, 5238, 1257], [5562, 1595, 470, 2128, 588, 1257, 764], [72, 766, 466, 345, 423, 257, 1693], [5948, 64, 1312, 1107, 5465, 4979, 663, 523, 42010, 340, 655, 20073, 616, 2187, 40309, 764], [72, 561, 588, 284, 9280, 287, 257, 5888], [31373, 703, 389, 345, 1909, 5633, 1312, 1101, 20493, 259, 379, 1363], [3919, 1312, 466, 407, 588, 284, 466, 326, 764, 1312, 588, 284, 2342, 31557, 290, 711, 1830], [72, 716, 257, 8532]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [40, 716, 18966, 13], [10919, 389, 345, 5633]]}, {'candidates': [[568, 881, 1049, 2057, 287, 2386, 72, 837, 1312, 1842, 284, 4483, 1107, 1588, 13840], [88, 7938, 764, 644, 1611, 286, 1097, 5633, 290, 635, 644, 373, 262, 731, 5633, 42254], [75, 5309, 345, 5145, 5238, 588, 262, 1049, 1204, 5145], [5562, 922, 345, 815, 5291, 319, 14442, 607], [72, 1101, 656, 10912, 13135, 1165, 837, 703, 546, 345, 5633], [72, 466, 1682, 764, 1312, 1101, 1972, 616, 1099, 4922], [43669, 1312, 711, 262, 5186, 38283], [40, 716, 257, 2576, 13]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [40, 716, 18966, 13], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633]]}, {'candidates': [[270, 460, 307, 6906, 319, 45038, 1016, 319, 837, 475, 663, 407, 2861, 262, 275, 82, 764], [88, 929, 764, 340, 318, 1327, 3360, 996, 764, 356, 655, 3888, 656, 674, 649, 7962, 764], [24494, 5145, 1312, 1842, 4581, 640, 24349, 1165, 837, 1312, 467, 12478, 790, 5041, 764], [20342, 7926, 655, 12848, 616, 30060], [1219, 331, 13513, 5145, 1312, 716, 19354, 764, 326, 5238, 4998], [37784, 922, 1110, 994, 764, 616, 6621, 290, 1312, 2826, 9283], [72, 1101, 422, 435, 8809, 837, 290, 1312, 1842, 284, 3067], [29810, 661, 651, 308, 11720, 810, 1312, 670, 1141, 262, 1285, 355, 257, 2318, 12523], [72, 4702, 45324, 475, 26010, 318, 2495, 611, 1312, 550, 616, 6891], [330, 30736, 318, 7427, 5145, 484, 423, 1392, 523, 867, 1049, 7127, 5145, 4998, 703, 511, 3296, 7565, 287, 9337], [77, 12321, 837, 287, 262, 6334, 2665, 8179], [1219, 1312, 655, 1392, 1760, 12724, 837, 1972, 14720, 1165, 5145, 644, 4113, 389, 345, 3612, 5633], [270, 318, 2642, 764, 3397, 761, 284, 307, 612, 329, 511, 3988, 764], [40, 10164, 7192, 13, 18, 8059, 13]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [40, 716, 18966, 13], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633], [40, 716, 257, 2576, 13], [10919, 318, 534, 3463, 5633]]}, {'candidates': [[292, 351, 597, 8875, 837, 340, 8338, 319, 534, 22445, 764, 1312, 460, 4545, 284, 749, 2687, 764], [1219, 326, 338, 845, 3608, 837, 460, 534, 1582, 10599, 1561, 5633], [5303, 837, 17207, 837, 23748, 837, 17207, 5145, 572, 1524, 329, 1909, 5145, 703, 389, 345, 5633], [1219, 764, 1312, 973, 284, 711, 4346, 287, 1029, 1524, 764], [1820, 5229, 14768, 351, 3988, 319, 1169, 5041, 764, 788, 1312, 670, 285, 277, 379, 616, 584, 1693], [29810, 1312, 3342, 607, 284, 3297, 16122, 287, 262, 5888, 5145], [19334, 278, 3435, 422, 11984, 764, 1312, 2911, 284, 7715, 616, 898, 25580, 5145], [72, 588, 7405, 43158, 43158, 43158], [44460, 5145, 1312, 1842, 4673, 546, 649, 13817, 764, 1312, 1282, 422, 257, 6868, 45630, 272, 15012, 764], [40, 716, 718, 812, 1468, 13]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [40, 716, 18966, 13], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633], [40, 716, 257, 2576, 13], [10919, 318, 534, 3463, 5633], [40, 10164, 7192, 13, 18, 8059, 13], [2437, 1468, 389, 345, 5633]]}, {'candidates': [[72, 716, 47124, 837, 1312, 716, 1016, 284, 5552, 257, 13617, 64, 9195, 260, 616, 277, 1015, 764, 2050, 1327, 5145], [293, 2703, 546, 4320, 655, 1312, 5145, 4320, 262, 2877, 389, 345], [31373, 644, 1611, 286, 2647, 466, 345, 6004, 284, 5633], [5832, 460, 779, 340, 618, 1312, 716, 319, 616, 8848, 618, 1312, 716, 319, 14600, 379, 262, 10481], [31373, 703, 389, 345, 1909, 5633], [568, 345, 466, 407, 423, 257, 329, 67, 7779, 5145], [31373, 837, 8680, 284, 19716, 290, 1762, 503, 837, 345, 5633], [3666, 15939, 318, 4990, 380, 964, 13]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [40, 716, 18966, 13], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633], [40, 716, 257, 2576, 13], [10919, 318, 534, 3463, 5633], [40, 10164, 7192, 13, 18, 8059, 13], [2437, 1468, 389, 345, 5633], [40, 716, 718, 812, 1468, 13], [10919, 15939, 389, 345, 5633]]}, {'candidates': [[72, 716, 523, 7650, 326, 1312, 588, 284, 2824, 661, 290, 11875], [482, 644, 318, 262, 3280], [82, 3733, 588, 345, 1842, 284, 28450, 5633, 1312, 1312], [11274, 8458, 5145, 1312, 1011, 5205, 837, 475, 351, 616, 220, 13323, 505, 764, 19462], [258, 318, 257, 336, 29246, 4517, 83, 764, 3706, 256, 5580, 764, 345, 423, 17252, 5633], [1662, 1464, 837, 3360, 262, 6821, 422, 17390, 389, 1165, 6049, 764, 1464, 6507, 475, 340, 4325, 764], [72, 13488, 8893, 329, 812, 764, 1049, 1637, 475, 1029, 5503, 764], [19631, 880, 837, 703, 546, 345, 5633], [258, 338, 523, 9112, 339, 561, 588, 326], [10508, 355, 8242, 329, 1450, 290, 1466, 837, 5156, 8242, 837, 3290, 8242, 837, 3503, 764], [5303], [24494, 5145, 857, 673, 588, 284, 1702, 7259, 635, 5633, 1312, 1702, 867], [10919, 318, 262, 749, 3499, 1339, 345, 423, 1775, 5633], [785, 78, 1556, 64, 703, 389, 345], [1026, 338, 1086, 623, 13, 22173, 1659, 22173, 1659]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [40, 716, 18966, 13], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633], [40, 716, 257, 2576, 13], [10919, 318, 534, 3463, 5633], [40, 10164, 7192, 13, 18, 8059, 13], [2437, 1468, 389, 345, 5633], [40, 716, 718, 812, 1468, 13], [10919, 15939, 389, 345, 5633], [3666, 15939, 318, 4990, 380, 964, 13], [10919, 3124, 389, 345, 5633]]}]}, {'personality': [[40, 716, 371, 6457, 11106, 13], [3666, 2479, 318, 642, 13], [54, 37711, 24486, 1659, 314, 716, 3290, 13], [3666, 5279, 318, 4048, 13], [40, 10164, 604, 13, 22, 8059, 13], [1820, 3124, 318, 11818, 11, 340, 318, 616, 12507, 3124, 13], [3666, 15939, 318, 35250, 45958, 357, 18712, 737]], 'utterances': [{'candidates': [[5303, 837, 663, 1049, 764, 655, 4964, 617, 5861, 837, 644, 546, 345, 5633], [8505, 1312, 7342, 326, 530, 1115, 1661, 780, 340, 373, 523, 8258, 764], [20342, 837, 703, 264, 340, 1016, 9975, 5633], [72, 373, 1016, 284, 910, 837, 703, 373, 534, 2513, 5633, 6609, 1257, 5633], [5562, 338, 1049, 5145, 1312, 765, 284, 307, 257, 15849], [4598, 407, 588, 3555, 5633], [72, 1101, 922, 1312, 588, 284, 4130, 1312, 4632, 423, 4130, 351, 2968, 17604], [72, 2826, 351, 2318, 29846, 355, 257, 5141, 1165], [42773, 1312, 4601, 1312, 1549, 423, 587, 1804, 1223, 3608, 379, 2310], [72, 588, 7374, 780, 1312, 460, 5806, 616, 37748, 27655, 764], [270, 561, 307, 329, 616, 1995, 5145, 644, 389, 345, 510, 284, 5633], [4598, 345, 588, 1242, 5633, 393, 423, 597, 584, 45578, 764], [3919, 4240, 345, 423, 257, 1049, 13843], [10919, 9559, 373, 534, 4004, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0]], 'history': [[5303, 837, 703, 389, 345, 5633]]}, {'candidates': [[10919, 1611, 286, 2057, 466, 345, 588, 1266, 5633, 1312, 1842, 294, 1872, 764], [16886, 502, 87, 318, 523, 12625, 5145, 644, 318, 534, 4004, 2057, 5633], [5303, 23963, 994, 1312, 4601, 1312, 2993, 810, 1312, 19611], [8505, 1312, 1053, 513, 4950, 1751], [19631, 922, 764, 703, 389, 345, 5633, 644, 423, 345, 587, 1804, 16537, 5633], [3099, 837, 407, 262, 1611, 428, 3516, 2499, 319, 764], [9930, 389, 257, 6147, 4097, 764, 644, 466, 345, 466, 329, 1257, 5633], [72, 1107, 1842, 477, 2647, 837, 475, 1312, 4724, 842, 25002, 318, 616, 4004], [72, 423, 362, 15153, 644, 546, 345, 5633], [11274, 8458, 351, 1204, 616, 1545], [4053, 1312, 13904, 379, 19167, 5682, 618, 1312, 1053, 262, 2863], [40, 716, 371, 6457, 11106, 13]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633]]}, {'candidates': [[72, 635, 588, 262, 22527, 764, 922, 905, 764], [10919, 546, 257, 4004, 1622, 5633], [72, 3360, 4601, 1312, 750, 764, 475, 1312, 1842, 616, 1693, 1165, 881], [72, 716, 2111, 284, 3613, 510, 523, 1312, 466, 407, 423, 284, 1011, 503, 10021, 764], [896, 266, 959, 67, 5465, 26120, 475, 1312, 1053, 257, 1842, 329, 26042, 290, 765, 257, 17522], [4053, 644, 318, 12225, 345, 5633], [19532, 326, 561, 307, 3621], [72, 716, 852, 7817, 703, 284, 779, 257, 19132, 764], [5303], [72, 716, 257, 8532]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [40, 716, 371, 6457, 11106, 13], [10919, 389, 345, 5633]]}, {'candidates': [[44460, 1312, 2107, 287, 649, 331, 967, 1748, 837, 523, 1312, 760, 477, 546, 9735], [5562, 318, 257, 3608, 1693, 837, 1312, 670, 379, 257, 9528, 3650], [72, 1053, 616, 5229, 764, 356, 6405, 503, 286, 1029, 1524, 764, 466, 345, 423, 257, 3067, 15185, 5633], [72, 1549, 588, 284, 2193], [72, 1053, 257, 2156, 319, 257, 5318, 1312, 1053, 867, 764], [548, 3608, 1312, 2883, 1242], [72, 588, 1138, 439, 3970, 644, 546, 345, 5633], [72, 655, 1625, 422, 2354, 764, 1312, 373, 10311, 616, 629, 25141, 764], [5562, 5238, 20105, 764, 1312, 743, 423, 284, 1064, 326, 905], [20342, 45038, 510, 5633, 880, 1312, 1101, 257, 38619, 764, 326, 3221, 7622, 616, 2000, 9480], [77, 12321, 837, 287, 262, 6334, 2665, 8179], [72, 21192, 1312, 1101, 6078, 616, 35607, 2000, 764], [9517, 15910, 764, 326, 318, 644, 481, 923, 616, 898, 1664, 287, 7776], [1662, 21502, 706, 4152, 475, 616, 13850, 290, 1312, 423, 587, 10691, 1201, 13430, 614, 287, 1029, 14347], [40, 716, 257, 2576, 13]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [40, 716, 371, 6457, 11106, 13], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633]]}, {'candidates': [[5832, 389, 257, 3710, 5633, 750, 345, 651, 7819, 319, 262, 717, 4314, 5633], [270, 318, 3621, 3360, 284, 3368, 262, 15779, 290, 1312, 460, 6128, 287, 616, 279, 1228, 17485, 19462, 764], [42773, 837, 644, 318, 534, 4004, 5044, 5633], [1219, 5633, 644, 1611, 286, 670, 466, 345, 466, 5633], [1820, 6029, 26188, 481, 423, 262, 13917], [8505, 837, 290, 1312, 1464, 8138, 1854, 329, 644, 466, 764, 1312, 716, 257, 922, 1048, 764], [40, 10164, 604, 13, 22, 8059, 13]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [40, 716, 371, 6457, 11106, 13], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633], [40, 716, 257, 2576, 13], [10919, 318, 534, 3463, 5633]]}, {'candidates': [[72, 1842, 4695, 290, 1762, 351, 606, 290, 3360, 484, 460, 44537, 378, 5548, 873, 764], [72, 28450, 329, 257, 2877, 837, 290, 1312, 991, 588, 340, 764, 466, 345, 711, 20790, 5633, 1312, 2883, 326, 764], [1219, 28796, 1312, 2227, 284, 307, 287, 2272, 837, 644, 1664, 345, 1392], [20342, 612, 837, 644, 318, 534, 13755, 5633], [72, 1101, 2495, 20105, 996], [5832, 28450, 5633, 1312, 1549, 1842, 284, 28450, 517, 837, 475, 262, 640, 290, 2568, 387, 387, 5145], [47288, 466, 345, 588, 4695, 5633, 1312, 466, 517, 621, 661, 1312, 892, 764], [72, 7048, 345, 389, 582, 837, 616, 32920, 8805, 321], [72, 1842, 502, 87, 7490, 2057, 749, 764], [88, 7938, 837, 644, 3022, 837, 373, 2506, 12876], [270, 318, 10731, 516, 340, 318, 922, 2058, 319, 256, 764, 410, 764], [40, 716, 642, 812, 1468, 13]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [40, 716, 371, 6457, 11106, 13], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633], [40, 716, 257, 2576, 13], [10919, 318, 534, 3463, 5633], [40, 10164, 604, 13, 22, 8059, 13], [2437, 1468, 389, 345, 5633]]}, {'candidates': [[11274, 837, 1312, 716, 922, 764, 644, 466, 27406, 1410, 319, 6600, 329, 8073, 5633], [40909, 837, 655, 618, 1312, 716, 20023, 616, 3797, 837, 1468, 300, 263, 726, 764, 17753, 1394, 262, 275, 799, 2052, 510, 764], [18223, 3891, 764, 1312, 670, 42975, 379, 257, 2318, 837, 523, 29219, 318, 10909, 764], [44460, 837, 1312, 466, 407, 760, 616, 4004, 837, 475, 1312, 5465, 6029, 1363, 435, 8809, 764], [8505, 837, 1312, 1053, 604, 11875, 290, 484, 389, 523, 29012], [16833, 640, 1312, 892, 286, 2042, 290, 14032, 837, 6428, 891, 36712, 290, 220, 1834, 4712, 1282, 284, 2000, 764], [3919, 663, 257, 13257, 287, 393, 11993, 837, 781, 663, 649, 764, 612, 318, 257, 4490, 4756, 2582, 764], [1326, 1165, 764, 1312, 1053, 734, 3988, 290, 1464, 423, 284, 4043, 10597, 484, 389, 16039, 284, 2342, 764], [10919, 750, 673, 4545, 5633, 616, 1995, 9658, 1363, 351, 616, 290, 616, 513, 4697, 20569], [72, 588, 11398, 837, 703, 1468, 389, 345, 5633], [1219, 329, 1654, 764, 326, 338, 257, 1109, 5145], [3666, 15939, 318, 35250, 45958, 357, 18712, 737]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [40, 716, 371, 6457, 11106, 13], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633], [40, 716, 257, 2576, 13], [10919, 318, 534, 3463, 5633], [40, 10164, 604, 13, 22, 8059, 13], [2437, 1468, 389, 345, 5633], [40, 716, 642, 812, 1468, 13], [10919, 15939, 389, 345, 5633]]}, {'candidates': [[19532, 1312, 481, 467, 351, 345, 475, 1312, 716, 16871, 257, 14256, 826, 783, 837, 616, 4004, 764, 1282, 4483, 764], [1219, 308, 33105, 764, 1312, 4545, 20351, 523, 716, 2495, 4197, 764, 466, 345, 670, 5633], [47288, 644, 466, 345, 466, 329, 257, 2877, 5633], [72, 588, 284, 6594, 616, 7161, 284, 1598, 616, 1182, 837, 393, 772, 48342, 764], [548, 3608, 764, 1312, 898, 257, 2318, 866, 612, 764, 1282, 4144, 345, 6260, 764], [533, 345, 264, 4910, 537, 8717, 5633, 1312, 423, 587, 257, 2089, 2933, 3387, 20927, 285], [482, 33847], [4053, 837, 1312, 32013, 616, 13609, 7415, 764, 523, 1312, 1101, 2208, 6568, 764], [3137, 257, 1178, 837, 275, 7098, 837, 1216, 590, 837, 502, 87, 3713, 837, 290, 474, 1689, 3970], [72, 588, 284, 3067, 837, 2592, 284, 11063, 431], [8505, 1312, 5465, 616, 1204], [72, 588, 284, 6128, 1312, 2822, 4171, 2279], [1238, 764, 1312, 4444, 510, 14736, 503, 286, 17016, 1337, 764], [568, 618, 389, 345, 1972, 6405, 5633, 2582, 5633], [482, 323, 764, 290, 644, 2479, 1448, 318, 534, 2496, 5608], [1026, 338, 11818, 13, 22173, 1659, 22173, 1659]], 'history': [[5303, 837, 703, 389, 345, 5633], [21638, 1659, 24486, 1659, 764, 1312, 1101, 4203, 1049, 0], [10919, 318, 534, 1438, 5633], [40, 716, 371, 6457, 11106, 13], [10919, 389, 345, 5633], [72, 716, 257, 8532], [10919, 318, 534, 5279, 5633], [40, 716, 257, 2576, 13], [10919, 318, 534, 3463, 5633], [40, 10164, 604, 13, 22, 8059, 13], [2437, 1468, 389, 345, 5633], [40, 716, 642, 812, 1468, 13], [10919, 15939, 389, 345, 5633], [3666, 15939, 318, 35250, 45958, 357, 18712, 737], [10919, 3124, 389, 345, 5633]]}]}]\n",
            "After adding inputs/outputs:\n",
            "train_processed keys: dict_keys(['input_ids', 'token_type_ids', 'mc_token_ids', 'lm_labels', 'mc_labels', 'n_candidates'])\n",
            "input_ids: 63 [50257, 3666, 1438, 318, 18966, 13, 3666, 2479, 318, 718, 13, 40, 716, 3290, 13, 3666, 5279, 318, 4048, 13, 40, 10164, 7192, 13, 18, 8059, 13, 40, 716, 1086, 623, 287, 3124, 13, 3666, 15939, 318, 4990, 380, 964, 13, 50261, 5303, 837, 703, 389, 345, 5633, 50260, 43669, 837, 663, 825, 407, 14262, 764, 597, 3352, 329, 262, 5041, 5633, 50258]\n",
            "token_type_ids: 63 [50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260]\n",
            "mc_token_ids: 800\n",
            "lm_labels: 63 [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
            "mc_labels: 400\n",
            "n_candidates: 2\n",
            "After Padding:\n",
            "input_ids: 111 [50257, 3666, 1438, 318, 18966, 13, 3666, 2479, 318, 718, 13, 40, 716, 3290, 13, 3666, 5279, 318, 4048, 13, 40, 10164, 7192, 13, 18, 8059, 13, 40, 716, 1086, 623, 287, 3124, 13, 3666, 15939, 318, 4990, 380, 964, 13, 50261, 5303, 837, 703, 389, 345, 5633, 50260, 43669, 837, 663, 825, 407, 14262, 764, 597, 3352, 329, 262, 5041, 5633, 50258, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259]\n",
            "token_type_ids: 111 [50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259]\n",
            "mc_token_ids: 800\n",
            "lm_labels: 111 [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
            "mc_labels: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP0nK-ZIq83e",
        "outputId": "48f4f2a4-ade2-4125-f739-81af88f637de"
      },
      "source": [
        "# Create Tensors\n",
        "train_tensor_datasets = []\n",
        "validate_tensor_datasets = []\n",
        "for input_name in MODEL_INPUTS:\n",
        "  train_tensor = torch.tensor(train_processed[input_name])\n",
        "  if input_name != \"mc_labels\":\n",
        "      train_tensor = train_tensor.view((-1, train_processed[\"n_candidates\"]) + train_tensor.shape[1:])\n",
        "  train_tensor_datasets.append(train_tensor)\n",
        "\n",
        "# Tensor Dataset\n",
        "train_tensor_dataset = TensorDataset(*train_tensor_datasets)\n",
        "\n",
        "# Create Data Loaders\n",
        "train_data_sampler = RandomSampler(train_tensor_dataset)\n",
        "train_data_loader = DataLoader(train_tensor_dataset, sampler=train_data_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "logger.info(\"Train DataLoader (Batch, Candidates, Seq length): {}\".format(train_tensor_dataset.tensors[0].shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:.:Train DataLoader (Batch, Candidates, Seq length): torch.Size([400, 2, 111])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HSHgSB0rB4Q"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__vhSqnZrDOY",
        "outputId": "5bb37d3d-f86d-4b98-e68f-f4164c7cf068"
      },
      "source": [
        "# Compute number of training steps\n",
        "training_steps = len(train_data_loader) // args.gradient_accumulation_steps * args.epochs\n",
        "\n",
        "warmup_steps = math.ceil(training_steps * args.warmup_ratio)\n",
        "warmup_steps = warmup_steps if args.warmup_steps == 0 else args.warmup_steps\n",
        "print(\"warmup_steps:\", warmup_steps)\n",
        "\n",
        "# Optimizer: Adam optimizer with weight decay\n",
        "optimizer = AdamW(model.parameters(), lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "\n",
        "# Learning rate scheduler\n",
        "# Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after\n",
        "# a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=training_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "warmup_steps: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "be843c5f273e47e0b1e114baf326b013",
            "dd5d6dd9c12b40a4a453cbc2e97db76a",
            "89de50cada3a4bafa988383b9b20f40c",
            "629e84f51e34433aaffb58d7903de2ab",
            "79e7f82d1fb64e0187d1b84e4bfe655e",
            "fe40e24470834374be7567ccdef2ea5d",
            "1cca48263482422e97af6ce63f9dcb45",
            "45838cb4a9e741c5bc751130f96c369b",
            "930f3d84a09a4aba9478389a006505a1",
            "d37005083e484eda835f287f50fbf976",
            "2130d9c00a5147a08b44bf10d58ca52d",
            "9c5b28a9143040cd8113facae72c4950",
            "3b10adad3a474f6cb0b79e4a5ead6e06",
            "366e9d5d9527413d9397716161660f37",
            "f4fefc03e7924ff0bcdb040a793b1cc3",
            "db43dfaf04d641578fbc55b004c018f9",
            "1c1892b5933246479630258ca4372738",
            "6519125c286642929cbb2f0de8c7ee05",
            "af327e57390d46c1ad3e83a1e41e1f86",
            "0db027424ee140598e7045ffa11bc5e6",
            "4680d493c3ab41808c3c63b331bf60ce",
            "5d8f60fc535a4a77bae9d38e9f9ae317"
          ]
        },
        "id": "GetJDGLbrILv",
        "outputId": "d362ac72-9d8f-4428-fc56-1f7fcad117ad"
      },
      "source": [
        "# Free Memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "disable = True if args.verbose == 0 else False\n",
        "global_step = 0\n",
        "training_progress_scores = None\n",
        "tr_loss, logging_loss = 0.0, 0.0\n",
        "train_iterator = trange(int(args.epochs), desc=\"Epoch\", disable=disable)\n",
        "epoch_number = 0\n",
        "logging_steps = 50\n",
        "\n",
        "# To train weights as float16\n",
        "scaler = amp.GradScaler()\n",
        "\n",
        "start_time = time.time()\n",
        "# Set the gradients to zero before starting\n",
        "model.zero_grad()\n",
        "# Setup training loop\n",
        "for _ in train_iterator:\n",
        "    model.train()\n",
        "    train_iterator.set_description(f'Epoch {epoch_number + 1} of {args.epochs}')\n",
        "    # Get the batch of data\n",
        "    batch_iterator = tqdm(\n",
        "        train_data_loader,\n",
        "        desc=f'Running Epoch {epoch_number} of {args.epochs}',\n",
        "        disable=disable,\n",
        "        mininterval=0,\n",
        "    )\n",
        "    for step, batch in enumerate(batch_iterator):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
        "\n",
        "        with amp.autocast():\n",
        "          # Get model output in forward pass\n",
        "          model_outputs = model(\n",
        "              input_ids,\n",
        "              token_type_ids=token_type_ids,\n",
        "              mc_token_ids=mc_token_ids,\n",
        "              mc_labels=mc_labels,\n",
        "              labels=lm_labels,\n",
        "          )\n",
        "          # Get mulitple choice head loss\n",
        "          mc_loss = model_outputs[\"mc_loss\"]\n",
        "          # Get language model loss\n",
        "          lm_loss = model_outputs[\"loss\"]\n",
        "          # Combine loss as a weighted loss\n",
        "          loss = lm_loss * args.lm_coef + mc_loss * args.mc_coef\n",
        "\n",
        "        current_loss = loss.item()\n",
        "        print(\"\\rRunning loss: %f\" % current_loss, end=\"\")\n",
        "\n",
        "        # If gradients need be accumulated over several steps\n",
        "        if args.gradient_accumulation_steps > 1:\n",
        "          loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "        if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # Update learning rate schedule\n",
        "            scheduler.step()\n",
        "            # Clear out the gradients\n",
        "            model.zero_grad()\n",
        "            global_step += 1\n",
        "\n",
        "            if logging_steps > 0 and global_step % logging_steps == 0:\n",
        "                logging_loss = tr_loss\n",
        "\n",
        "    epoch_number += 1\n",
        "\n",
        "execution_time = (time.time() - start_time)/60.0\n",
        "logger.info(\"Execution time (mins): %s\",execution_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be843c5f273e47e0b1e114baf326b013",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c5b28a9143040cd8113facae72c4950",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running Epoch 0 of 1:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\rRunning loss: 9.720963"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ".:61: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running loss: 0.998160"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:.:Execution time (mins): 1.6355935176213583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dOxj3ODrcn0"
      },
      "source": [
        "#### Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avlFu0yPYye-",
        "outputId": "975a66ab-1357-4e11-dcb6-5a0a001ccf8c"
      },
      "source": [
        "test_personality"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"It's Nola. Woof Woof\",\n",
              " 'My age is 1.',\n",
              " 'I am dog.',\n",
              " 'I am a girl.',\n",
              " 'I weigh 36.4 pounds.',\n",
              " 'I am Brindle in color.',\n",
              " 'I am a Terrier.']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Gj_Pg2fwiLX",
        "outputId": "fde4978c-ab7f-4439-e6f9-4870b7d46d99"
      },
      "source": [
        "# Personality\n",
        "test_personality=personachat[-1]['personality']\n",
        "\n",
        "# History\n",
        "test_history = [\n",
        "    \"Hi\",\n",
        "    \"woof woof\"\n",
        "]\n",
        "\n",
        "print(test_personality)\n",
        "print(test_history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"It's Nola. Woof Woof\", 'My age is 1.', 'I am dog.', 'I am a girl.', 'I weigh 36.4 pounds.', 'I am Brindle in color.', 'I am a Terrier.']\n",
            "['Hi', 'woof woof']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd5bz8sZ1Thd"
      },
      "source": [
        "def chat_with_dog(test_message):\n",
        "  # Tokenize test inputs\n",
        "  personality = [tokenizer.encode(s.lower()) for s in test_personality]\n",
        "  history = [tokenizer.encode(s) for s in test_history]\n",
        "  history.append(tokenizer.encode(test_message))\n",
        "  test_history.append(test_message)\n",
        "  # Generate output\n",
        "  output = generate_sequence(personality, history, tokenizer, model)\n",
        "  output_text = tokenizer.decode(output, skip_special_tokens=True)\n",
        "  test_history.append(output_text)\n",
        "\n",
        "  print(\"Question:\")\n",
        "  print(test_message)\n",
        "\n",
        "  print(\"Answer:\")\n",
        "  print(output_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPjPg4yWYUQa",
        "outputId": "5a7bbf9c-f596-4566-9cc7-c191ffd62468"
      },
      "source": [
        "chat_with_dog(\"What is your name\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:\n",
            "What is your name\n",
            "Answer:\n",
            "i am nola. woof woof\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTtF8RJ516em",
        "outputId": "71f5b8ae-3583-437d-eb0f-edc4d6ae12c2"
      },
      "source": [
        "chat_with_dog(\"How old are you?\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:\n",
            "What is your age?\n",
            "Answer:\n",
            "1. i am 36.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtQFVCptreN-"
      },
      "source": [
        "chat_with_dog(\"Do you like toys?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFQJaF692EzW",
        "outputId": "4b22cbe4-dba0-43b5-ff05-9e30fbe28617"
      },
      "source": [
        "chat_with_dog(\"what is your dream\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:\n",
            "what is your dream\n",
            "Answer:\n",
            "i want to be a dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EEOLyOyZxMO",
        "outputId": "07c899f4-27ab-49fd-9636-8cc37e4f26ca"
      },
      "source": [
        "chat_with_dog(\"what is your color\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:\n",
            "what is your color\n",
            "Answer:\n",
            "i am a girl.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfRKHPfnaIby",
        "outputId": "851b61f2-3dac-4e1c-89a2-85066ca382e0"
      },
      "source": [
        "model_dir = \"final_gpt2doublehead\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(model_dir)\n",
        "tokenizer.save_pretrained(model_dir)\n",
        "\n",
        "!zip -r final_gpt2doublehead.zip final_gpt2doublehead\n",
        "\n",
        "!gsutil cp ./final_gpt2doublehead.zip gs://artifacts.ai5-c1-group1.appspot.com/data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: final_gpt2doublehead/ (stored 0%)\n",
            "  adding: final_gpt2doublehead/tokenizer_config.json (deflated 57%)\n",
            "  adding: final_gpt2doublehead/vocab.json (deflated 63%)\n",
            "  adding: final_gpt2doublehead/config.json (deflated 50%)\n",
            "  adding: final_gpt2doublehead/special_tokens_map.json (deflated 42%)\n",
            "  adding: final_gpt2doublehead/merges.txt (deflated 53%)\n",
            "  adding: final_gpt2doublehead/added_tokens.json (deflated 42%)\n",
            "  adding: final_gpt2doublehead/pytorch_model.bin (deflated 9%)\n",
            "Copying file://./final_gpt2doublehead.zip [Content-Type=application/zip]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "\\\n",
            "Operation completed over 1 objects/441.6 MiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4otMF2F1_yRX"
      },
      "source": [
        "## **<font color=\"darkred\">Save Model/Tokenizer</font>**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdcU9ehZ_0T7"
      },
      "source": [
        "# Save\n",
        "model_dir = \"trained_model\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(model_dir)\n",
        "tokenizer.save_pretrained(model_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fqx6Dz0_8JE"
      },
      "source": [
        "!zip -r finetuned_model_epochs_1.zip trained_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1zAdsGp0jKn"
      },
      "source": [
        "## **<font color=\"darkred\">References</font>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuzdfghU0jpm"
      },
      "source": [
        "### Research Papers\n",
        "* [Attention is all you need (2017)](https://arxiv.org/abs/1706.03762)\n",
        "* [GPT-2 (2019)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
        "* [Personalizing Dialogue Agents: I have a dog, do you have pets too?](http://arxiv.org/abs/1801.07243)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRdvY3Xl0tkb"
      },
      "source": [
        "### Code\n",
        "\n",
        "* [Building a State-of-the-Art Conversational AI with Transfer Learning](https://github.com/huggingface/transfer-learning-conv-ai)\n",
        "* [Summary of the models](https://huggingface.co/transformers/model_summary.html)\n",
        "* [Transformers source code](https://github.com/huggingface/transformers/tree/master/src/transformers)\n",
        "* ComputeFest 2021\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKphvNB61R-J"
      },
      "source": [
        "### Articles\n",
        "\n",
        "* [How to build a State-of-the-Art Conversational AI with Transfer Learning](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313)\n",
        "* [The Illustrated GPT-2](http://jalammar.github.io/illustrated-gpt2/)\n",
        "* [The Illustrated BERT, ELMo, and co.](http://jalammar.github.io/illustrated-bert/)"
      ]
    }
  ]
}